{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "\n",
    "### Extensions with numpy and scipy: parameterized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import flip\n",
    "from scipy.signal import convolve2d, correlate2d\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the custom layer with parameters, for which the gradients could be retained. \n",
    "\n",
    "Special thanks to `Adam Paszke <https://github.com/apaszke>` for reasonable example used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScipyConv2dFunction(torch.autograd.Function):\n",
    "    @staticmethod #\n",
    "    def forward(ctx, my_input, my_filter, bias):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScipyConv2dFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, my_input, my_filter, bias):\n",
    "        # detach so we can cast to NumPy\n",
    "        my_input, my_filter, bias = my_input.detach(), my_filter.detach(), bias.detach()\n",
    "        result = correlate2d(my_input.numpy(), my_filter.numpy(), mode='valid')\n",
    "        result += bias.numpy()\n",
    "        ctx.save_for_backward(my_input, my_filter, bias)\n",
    "        return torch.as_tensor(result, dtype=my_input.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScipyConv2dFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, my_input, my_filter, bias):\n",
    "        # detach so we can cast to NumPy\n",
    "        my_input, my_filter, bias = my_input.detach(), my_filter.detach(), bias.detach()\n",
    "        result = correlate2d(my_input.numpy(), my_filter.numpy(), mode='valid')\n",
    "        result += bias.numpy()\n",
    "        ctx.save_for_backward(my_input, my_filter, bias)\n",
    "        return torch.as_tensor(result, dtype=my_input.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        my_input, my_filter, bias = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_bias = np.sum(grad_output, keepdims=True)\n",
    "        grad_my_input = convolve2d(grad_output, my_filter.numpy(), mode='full')\n",
    "        # the previous line can be expressed equivalently as:\n",
    "        # grad_my_input = correlate2d(grad_output, flip(flip(my_filter.numpy(), axis=0), axis=1), mode='full')\n",
    "        grad_my_filter = correlate2d(my_input.numpy(), grad_output, mode='valid')\n",
    "        return torch.from_numpy(grad_my_input), \n",
    "               torch.from_numpy(grad_my_filter).to(torch.float), \n",
    "               torch.from_numpy(grad_bias).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScipyConv2d(Module):\n",
    "    def __init__(self, my_filter_width, my_filter_height):\n",
    "        super(ScipyConv2d, self).__init__()\n",
    "        self.my_filter = Parameter(torch.randn(my_filter_width, my_filter_height))\n",
    "        self.bias = Parameter(torch.randn(1, 1))\n",
    "\n",
    "    def forward(self, my_input):\n",
    "        return ScipyConv2dFunction.apply(my_input, self.my_filter, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the gradients:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the gradients correct:  True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "moduleConv = ScipyConv2d(3, 3)\n",
    "\n",
    "my_input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]\n",
    "test = gradcheck(moduleConv, my_input, eps=1e-6, atol=1e-4)\n",
    "print(\"Are the gradients correct: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with tf",
   "language": "python",
   "name": "tf_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
