{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "\n",
    "### Extensions with numpy and scipy: parameter-less example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dummy function that does nothing but mutlipying the passed argument by $2.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a new class for our function. It should be based on `torch.autograd.Function` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoParameterNumpyFunction(torch.autograd.Function):\n",
    "    def forward(self, my_input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should posess `forward` and `backward` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods should take as input and return `torch.Tensor` type variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the __forward function__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoParameterNumpyFunction(torch.autograd.Function):\n",
    "    def forward(self, my_input):\n",
    "        numpy_input = my_input.detach().numpy()\n",
    "        return my_input.new(numpy_input * 2.5)\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with numpy smoothly the tensor should be detached and translated to `numpy.ndarray` type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the __backward function__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoParameterNumpyFunction(torch.autograd.Function):\n",
    "    def forward(self, my_input):\n",
    "        numpy_input = my_input.detach().numpy()\n",
    "        return my_input.new(numpy_input * 2.5)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        numpy_go = grad_output.numpy()\n",
    "        return grad_output.new(2.5 * numpy_go)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass provides gradient values, so no need of detaching them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define the function that applies our `NoParameterNumpyFunction` to the input data and check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(my_input):\n",
    "    return NoParameterNumpyFunction()(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      " tensor([[ 1.5014,  2.0003, -1.9374, -2.5834, -2.2760],\n",
      "        [-4.2454, -1.9620,  2.7827,  2.4674, -1.7583],\n",
      "        [-3.3536,  3.2906,  0.1479, -4.6234, -0.8852],\n",
      "        [ 4.0107,  0.1398,  3.8121,  1.5927, -1.5370],\n",
      "        [-1.0974, -1.5101,  2.7156,  1.2644, -2.6461]],\n",
      "       grad_fn=<NoParameterNumpyFunction>)\n",
      "Input: \n",
      " tensor([[ 0.6006,  0.8001, -0.7750, -1.0334, -0.9104],\n",
      "        [-1.6982, -0.7848,  1.1131,  0.9869, -0.7033],\n",
      "        [-1.3415,  1.3162,  0.0591, -1.8494, -0.3541],\n",
      "        [ 1.6043,  0.0559,  1.5249,  0.6371, -0.6148],\n",
      "        [-0.4390, -0.6040,  1.0863,  0.5058, -1.0584]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "my_input = torch.randn(5, 5, requires_grad=True)\n",
    "result = my_function(my_input)\n",
    "print('Result: \\n', result)\n",
    "result.backward(torch.randn(result.size()))\n",
    "print('Input: \\n', my_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same example with 'incorrect FFT' written by `Adam Paszke <https://github.com/apaszke>` is available below for your consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4602,  4.0459,  9.1712, 11.2801,  2.8586],\n",
      "        [ 9.1331, 10.2702,  2.3852,  9.6039,  6.9081],\n",
      "        [ 8.9842,  5.9188,  3.4215,  1.7207,  5.4507],\n",
      "        [ 6.7364,  3.4633, 17.2621,  8.7835,  4.2332],\n",
      "        [11.9287, 12.4571,  9.6719,  7.2652,  0.3746],\n",
      "        [ 6.7364,  7.7286,  6.8314,  7.5754,  4.2332],\n",
      "        [ 8.9842,  4.8460,  4.1552,  4.7081,  5.4507],\n",
      "        [ 9.1331,  6.1811, 11.5793,  3.3977,  6.9081]],\n",
      "       grad_fn=<BadFFTFunction>)\n",
      "tensor([[ 1.3022,  0.1690,  0.9718, -0.2892,  0.1146,  0.8799,  0.4793, -0.0734],\n",
      "        [-0.7867, -0.9935, -0.3931,  0.9037,  1.3205, -1.4621, -0.6775, -0.9220],\n",
      "        [-0.4453, -1.0873, -0.2218,  0.1006, -0.3520, -0.2610,  0.0036, -1.5339],\n",
      "        [ 1.2376, -0.1043, -1.9211,  1.0928,  0.4114,  0.2686, -1.0042, -2.1474],\n",
      "        [ 0.8602,  1.1203,  0.9459,  0.5719, -0.4241, -1.8078,  0.3367,  1.0221],\n",
      "        [ 1.7610,  0.5820, -1.1527, -0.4963,  0.3826,  2.6382,  0.0099, -1.9355],\n",
      "        [ 1.0803,  0.7274, -0.1693, -0.4731, -0.9928,  1.5430, -0.5012,  1.1373],\n",
      "        [-0.8455, -0.6559,  0.0135,  0.1724, -0.7056, -0.3476, -0.4390, -0.9985]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from numpy.fft import rfft2, irfft2\n",
    "\n",
    "\n",
    "class BadFFTFunction(torch.autograd.Function):\n",
    "\n",
    "    def forward(self, my_input):\n",
    "        numpy_input = my_input.detach().numpy()\n",
    "        result = abs(rfft2(numpy_input))\n",
    "        return my_input.new(result)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        numpy_go = grad_output.numpy()\n",
    "        result = irfft2(numpy_go)\n",
    "        return grad_output.new(result)\n",
    "\n",
    "# since this layer does not have any parameters, we can\n",
    "# simply declare this as a function, rather than as an nn.Module class\n",
    "\n",
    "\n",
    "def incorrect_fft(my_input):\n",
    "    return BadFFTFunction()(my_input)\n",
    "\n",
    "input = torch.randn(8, 8, requires_grad=True)\n",
    "result = incorrect_fft(input)\n",
    "print(result)\n",
    "result.backward(torch.randn(result.size()))\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with tf",
   "language": "python",
   "name": "tf_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
