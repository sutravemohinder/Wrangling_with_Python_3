{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, encoding_dim)\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = Autoencoder(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tItem Loss: 1.278752, Train Loss: 81.840157\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tItem Loss: 0.759653, Train Loss: 5578.821987\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tItem Loss: 0.658490, Train Loss: 10026.326561\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tItem Loss: 0.607910, Train Loss: 14175.130207\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tItem Loss: 0.600441, Train Loss: 18144.080215\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tItem Loss: 0.594678, Train Loss: 21992.800495\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tItem Loss: 0.561173, Train Loss: 25776.134773\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tItem Loss: 0.564663, Train Loss: 29486.388577\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tItem Loss: 0.560818, Train Loss: 33172.950706\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tItem Loss: 0.565808, Train Loss: 36837.129341\n",
      "\n",
      "Train set: Overall train loss: 40.6807\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tItem Loss: 0.553203, Train Loss: 35.405014\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tItem Loss: 0.595528, Train Loss: 3643.178856\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tItem Loss: 0.574054, Train Loss: 7246.946907\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tItem Loss: 0.568537, Train Loss: 10826.395626\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tItem Loss: 0.568102, Train Loss: 14395.204697\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tItem Loss: 0.560252, Train Loss: 17944.475410\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tItem Loss: 0.570191, Train Loss: 21457.958305\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tItem Loss: 0.554125, Train Loss: 24973.833900\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tItem Loss: 0.551810, Train Loss: 28481.769136\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tItem Loss: 0.527766, Train Loss: 31967.995863\n",
      "\n",
      "Train set: Overall train loss: 35.4443\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tItem Loss: 0.544117, Train Loss: 34.823467\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tItem Loss: 0.552251, Train Loss: 3530.515444\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tItem Loss: 0.545968, Train Loss: 7025.618078\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tItem Loss: 0.551453, Train Loss: 10513.932875\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tItem Loss: 0.557579, Train Loss: 13976.233660\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tItem Loss: 0.508670, Train Loss: 17438.707771\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tItem Loss: 0.535557, Train Loss: 20896.649914\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tItem Loss: 0.557745, Train Loss: 24358.054829\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tItem Loss: 0.526859, Train Loss: 27820.733850\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tItem Loss: 0.545671, Train Loss: 31301.602659\n",
      "\n",
      "Train set: Overall train loss: 34.7143\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tItem Loss: 0.537528, Train Loss: 34.401806\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tItem Loss: 0.531683, Train Loss: 3510.158875\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tItem Loss: 0.545747, Train Loss: 6962.482239\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tItem Loss: 0.531513, Train Loss: 10427.425308\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tItem Loss: 0.530585, Train Loss: 13879.284863\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tItem Loss: 0.533208, Train Loss: 17322.887314\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tItem Loss: 0.521143, Train Loss: 20765.399036\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tItem Loss: 0.551967, Train Loss: 24212.115942\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tItem Loss: 0.523605, Train Loss: 27644.333799\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tItem Loss: 0.506411, Train Loss: 31093.190004\n",
      "\n",
      "Train set: Overall train loss: 34.4841\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tItem Loss: 0.550350, Train Loss: 35.222374\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tItem Loss: 0.520671, Train Loss: 3502.396618\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tItem Loss: 0.530406, Train Loss: 6948.441597\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tItem Loss: 0.509555, Train Loss: 10371.065210\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tItem Loss: 0.517393, Train Loss: 13808.852762\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tItem Loss: 0.537171, Train Loss: 17232.995796\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tItem Loss: 0.546278, Train Loss: 20659.585884\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tItem Loss: 0.534139, Train Loss: 24076.908308\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tItem Loss: 0.537944, Train Loss: 27511.189983\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tItem Loss: 0.524450, Train Loss: 30923.858639\n",
      "\n",
      "Train set: Overall train loss: 34.2876\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tItem Loss: 0.558158, Train Loss: 35.722111\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tItem Loss: 0.542519, Train Loss: 3442.307199\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tItem Loss: 0.534477, Train Loss: 6868.266825\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tItem Loss: 0.518581, Train Loss: 10313.216425\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tItem Loss: 0.592990, Train Loss: 13719.087070\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tItem Loss: 0.528047, Train Loss: 17139.368372\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tItem Loss: 0.556350, Train Loss: 20580.376936\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tItem Loss: 0.522360, Train Loss: 24003.236990\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tItem Loss: 0.541156, Train Loss: 27404.833477\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tItem Loss: 0.514494, Train Loss: 30818.540314\n",
      "\n",
      "Train set: Overall train loss: 34.1852\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tItem Loss: 0.532150, Train Loss: 34.057617\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tItem Loss: 0.517148, Train Loss: 3446.561502\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tItem Loss: 0.513387, Train Loss: 6840.362604\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tItem Loss: 0.523098, Train Loss: 10269.099525\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tItem Loss: 0.518652, Train Loss: 13670.579134\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tItem Loss: 0.560957, Train Loss: 17090.016857\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tItem Loss: 0.514989, Train Loss: 20510.138672\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tItem Loss: 0.563808, Train Loss: 23931.962818\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tItem Loss: 0.540882, Train Loss: 27343.182587\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tItem Loss: 0.525127, Train Loss: 30774.557497\n",
      "\n",
      "Train set: Overall train loss: 34.1426\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tItem Loss: 0.533103, Train Loss: 34.118622\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tItem Loss: 0.509660, Train Loss: 3446.259720\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tItem Loss: 0.531481, Train Loss: 6851.930250\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tItem Loss: 0.529364, Train Loss: 10272.104565\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tItem Loss: 0.547158, Train Loss: 13683.243301\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tItem Loss: 0.539116, Train Loss: 17083.572466\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tItem Loss: 0.577762, Train Loss: 20509.957306\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tItem Loss: 0.520674, Train Loss: 23933.119972\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tItem Loss: 0.527588, Train Loss: 27347.472685\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tItem Loss: 0.523463, Train Loss: 30749.863531\n",
      "\n",
      "Train set: Overall train loss: 34.1175\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tItem Loss: 0.540067, Train Loss: 34.564304\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tItem Loss: 0.543913, Train Loss: 3435.269892\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tItem Loss: 0.531071, Train Loss: 6851.325171\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tItem Loss: 0.520803, Train Loss: 10267.334303\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tItem Loss: 0.528735, Train Loss: 13670.695480\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tItem Loss: 0.541646, Train Loss: 17107.657236\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tItem Loss: 0.494603, Train Loss: 20499.535332\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tItem Loss: 0.522170, Train Loss: 23900.179848\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tItem Loss: 0.548905, Train Loss: 27332.915033\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tItem Loss: 0.506949, Train Loss: 30736.604893\n",
      "\n",
      "Train set: Overall train loss: 34.0993\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tItem Loss: 0.548387, Train Loss: 35.096760\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tItem Loss: 0.546088, Train Loss: 3452.925396\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tItem Loss: 0.521847, Train Loss: 6855.504145\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tItem Loss: 0.513627, Train Loss: 10273.692640\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tItem Loss: 0.560263, Train Loss: 13698.485821\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tItem Loss: 0.543831, Train Loss: 17082.111380\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tItem Loss: 0.542589, Train Loss: 20497.453768\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tItem Loss: 0.496460, Train Loss: 23903.302305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [51200/60000 (85%)]\tItem Loss: 0.525665, Train Loss: 27318.304804\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tItem Loss: 0.558793, Train Loss: 30738.197338\n",
      "\n",
      "Train set: Overall train loss: 34.0870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    # train loss for each epoch\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tItem Loss: {:.6f}, Train Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), train_loss))\n",
    "            \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    print('\\nTrain set: Overall train loss: {:.4f}\\n'.format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), \"mnist_autoencoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(source_data, reconstructed_data, index):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "    axes[0].imshow(np.squeeze(source_data[index]), cmap='gray')\n",
    "    axes[0].set_title('Source image')\n",
    "    axes[1].imshow(np.squeeze(reconstructed_data[index]), cmap='gray')\n",
    "    axes[1].set_title('Reconstructed image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader) # get one batch of data\n",
    "source_data, _ = dataiter.next()\n",
    "\n",
    "reconstructed_data = model(source_data.view(source_data.size(0), -1))\n",
    "reconstructed_data = reconstructed_data.view(batch_size, 1, 28, 28)\n",
    "reconstructed_data = reconstructed_data.detach().numpy()\n",
    "\n",
    "source_data = source_data.numpy()\n",
    "\n",
    "show_results(source_data, reconstructed_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 with xgboost",
   "language": "python",
   "name": "py3_with_xgboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
